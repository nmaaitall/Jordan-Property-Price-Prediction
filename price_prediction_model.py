import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings

warnings.filterwarnings('ignore')

print("=" * 70)
print("ğŸ¤– Ù†Ø¸Ø§Ù… ØªÙˆÙ‚Ø¹ Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„Ø£Ø±Ø¯Ù† - Machine Learning")
print("=" * 70)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\nğŸ“‚ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
df = pd.read_csv('jordan_properties.csv')
print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df)} Ø¹Ù‚Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­!\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2ï¸âƒ£ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("ğŸ”§ Ø¬Ø§Ø±ÙŠ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ù…Ù† Ù†Øµ Ù„Ø£Ø±Ù‚Ø§Ù…
le = LabelEncoder()
df['Ø§Ù„Ù…Ù†Ø·Ù‚Ø©_Ø±Ù‚Ù…'] = le.fit_transform(df['Ø§Ù„Ù…Ù†Ø·Ù‚Ø©'])

# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø®ØµØ§Ø¦Øµ (Features) ÙˆØ§Ù„Ù‡Ø¯Ù (Target)
features = ['Ø§Ù„Ù…Ø³Ø§Ø­Ø©_Ù…ØªØ±', 'Ø¹Ø¯Ø¯_Ø§Ù„ØºØ±Ù', 'Ø¹Ø¯Ø¯_Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª', 'Ø¹Ù…Ø±_Ø§Ù„Ø¨Ù†Ø§Ø¡_Ø³Ù†ÙˆØ§Øª',
            'Ø·Ø§Ø¨Ù‚', 'ÙŠÙˆØ¬Ø¯_Ù…ØµØ¹Ø¯', 'ÙŠÙˆØ¬Ø¯_Ù…ÙˆÙ‚Ù', 'ÙŠÙˆØ¬Ø¯_Ø­Ø¯ÙŠÙ‚Ø©',
            'ÙŠÙˆØ¬Ø¯_ØªØ¯ÙØ¦Ø©_Ù…Ø±ÙƒØ²ÙŠØ©', 'Ù‚Ø±Ø¨_Ø§Ù„Ø®Ø¯Ù…Ø§Øª', 'Ø§Ù„Ù…Ù†Ø·Ù‚Ø©_Ø±Ù‚Ù…']

X = df[features]
y = df['Ø§Ù„Ø³Ø¹Ø±_Ø¯ÙŠÙ†Ø§Ø±']

print(f"âœ… Ø¹Ø¯Ø¯ Ø§Ù„Ø®ØµØ§Ø¦Øµ: {len(features)}")
print(f"âœ… Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {features}\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3ï¸âƒ£ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("âœ‚ï¸ Ø¬Ø§Ø±ÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"âœ… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {len(X_train)} Ø¹Ù‚Ø§Ø±")
print(f"âœ… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {len(X_test)} Ø¹Ù‚Ø§Ø±\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4ï¸âƒ£ Ø¨Ù†Ø§Ø¡ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("=" * 70)
print("ğŸ—ï¸ Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...")
print("=" * 70)

print("\n1ï¸âƒ£ Linear Regression...")
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)
print("   âœ… ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨")

print("\n2ï¸âƒ£ Random Forest...")
rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
print("   âœ… ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨")

print("\n3ï¸âƒ£ Gradient Boosting...")
gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb_model.fit(X_train, y_train)
gb_pred = gb_model.predict(X_test)
print("   âœ… ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5ï¸âƒ£ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n" + "=" * 70)
print("ğŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
print("=" * 70)

models_results = []

for name, predictions in [('Linear Regression', lr_pred),
                          ('Random Forest', rf_pred),
                          ('Gradient Boosting', gb_pred)]:
    mae = mean_absolute_error(y_test, predictions)
    rmse = np.sqrt(mean_squared_error(y_test, predictions))
    r2 = r2_score(y_test, predictions)

    models_results.append({
        'Model': name,
        'MAE': mae,
        'RMSE': rmse,
        'R2_Score': r2
    })

    print(f"\nğŸ”¹ {name}:")
    print(f"   MAE: {mae:,.0f} Ø¯ÙŠÙ†Ø§Ø±")
    print(f"   RMSE: {rmse:,.0f} Ø¯ÙŠÙ†Ø§Ø±")
    print(f"   RÂ² Score: {r2:.4f} ({r2 * 100:.2f}%)")

results_df = pd.DataFrame(models_results)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6ï¸âƒ£ Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
best_model_name = results_df.loc[results_df['R2_Score'].idxmax(), 'Model']
print("\n" + "=" * 70)
print(f"ğŸ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_model_name}")
print("=" * 70)

if best_model_name == 'Linear Regression':
    best_model = lr_model
    best_pred = lr_pred
elif best_model_name == 'Random Forest':
    best_model = rf_model
    best_pred = rf_pred
else:
    best_model = gb_model
    best_pred = gb_pred

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7ï¸âƒ£ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø®ØµØ§Ø¦Øµ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if best_model_name in ['Random Forest', 'Gradient Boosting']:
    print("\nğŸ“ˆ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø®ØµØ§Ø¦Øµ ÙÙŠ Ø§Ù„ØªÙˆÙ‚Ø¹:")
    feature_importance = pd.DataFrame({
        'Feature': features,
        'Importance': best_model.feature_importances_
    }).sort_values('Importance', ascending=False)

    print(feature_importance.to_string(index=False))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 8ï¸âƒ£ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\nğŸ“Š Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©...")

fig = plt.figure(figsize=(16, 10))

plt.subplot(2, 3, 1)
plt.bar(results_df['Model'], results_df['R2_Score'], color=['skyblue', 'lightgreen', 'coral'])
plt.title('Model Comparison - RÂ² Score', fontweight='bold', fontsize=12)
plt.ylabel('RÂ² Score')
plt.ylim([0, 1])
plt.xticks(rotation=15, ha='right')
plt.grid(alpha=0.3, axis='y')

plt.subplot(2, 3, 2)
plt.bar(results_df['Model'], results_df['MAE'], color=['gold', 'lightblue', 'pink'])
plt.title('Model Comparison - MAE', fontweight='bold', fontsize=12)
plt.ylabel('MAE (JOD)')
plt.xticks(rotation=15, ha='right')
plt.grid(alpha=0.3, axis='y')

plt.subplot(2, 3, 3)
plt.scatter(y_test, best_pred, alpha=0.6, color='purple')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.title(f'Actual vs Predicted - {best_model_name}', fontweight='bold', fontsize=12)
plt.xlabel('Actual Price (JOD)')
plt.ylabel('Predicted Price (JOD)')
plt.grid(alpha=0.3)

plt.subplot(2, 3, 4)
errors = y_test - best_pred
plt.hist(errors, bins=50, color='teal', edgecolor='black', alpha=0.7)
plt.title('Prediction Error Distribution', fontweight='bold', fontsize=12)
plt.xlabel('Error (JOD)')
plt.ylabel('Frequency')
plt.grid(alpha=0.3, axis='y')

if best_model_name in ['Random Forest', 'Gradient Boosting']:
    plt.subplot(2, 3, 5)
    top_features = feature_importance.head(10)
    plt.barh(range(len(top_features)), top_features['Importance'], color='orange', alpha=0.7)
    plt.yticks(range(len(top_features)), top_features['Feature'], fontsize=9)
    plt.title('Top 10 Feature Importance', fontweight='bold', fontsize=12)
    plt.xlabel('Importance')
    plt.grid(alpha=0.3, axis='x')

plt.subplot(2, 3, 6)
percentage_error = np.abs((y_test - best_pred) / y_test) * 100
plt.hist(percentage_error, bins=50, color='salmon', edgecolor='black', alpha=0.7)
plt.title('Percentage Error Distribution', fontweight='bold', fontsize=12)
plt.xlabel('Error (%)')
plt.ylabel('Frequency')
plt.axvline(percentage_error.mean(), color='red', linestyle='--', linewidth=2,
            label=f'Mean: {percentage_error.mean():.1f}%')
plt.legend()
plt.grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('model_evaluation.png', dpi=300, bbox_inches='tight')
print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© ÙÙŠ: model_evaluation.png")
plt.show()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 9ï¸âƒ£ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n" + "=" * 70)
print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø£Ù…Ø«Ù„Ø© ÙˆØ§Ù‚Ø¹ÙŠØ©")
print("=" * 70)

example_1 = pd.DataFrame({
    'Ø§Ù„Ù…Ø³Ø§Ø­Ø©_Ù…ØªØ±': [150],
    'Ø¹Ø¯Ø¯_Ø§Ù„ØºØ±Ù': [3],
    'Ø¹Ø¯Ø¯_Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª': [2],
    'Ø¹Ù…Ø±_Ø§Ù„Ø¨Ù†Ø§Ø¡_Ø³Ù†ÙˆØ§Øª': [5],
    'Ø·Ø§Ø¨Ù‚': [3],
    'ÙŠÙˆØ¬Ø¯_Ù…ØµØ¹Ø¯': [1],
    'ÙŠÙˆØ¬Ø¯_Ù…ÙˆÙ‚Ù': [1],
    'ÙŠÙˆØ¬Ø¯_Ø­Ø¯ÙŠÙ‚Ø©': [0],
    'ÙŠÙˆØ¬Ø¯_ØªØ¯ÙØ¦Ø©_Ù…Ø±ÙƒØ²ÙŠØ©': [1],
    'Ù‚Ø±Ø¨_Ø§Ù„Ø®Ø¯Ù…Ø§Øª': [8],
    'Ø§Ù„Ù…Ù†Ø·Ù‚Ø©_Ø±Ù‚Ù…': [le.transform(['Ø¹Ø¨Ø¯ÙˆÙ†'])[0]]
})

predicted_price_1 = best_model.predict(example_1)[0]
print(f"\nğŸ  Ù…Ø«Ø§Ù„ 1: Ø´Ù‚Ø© Ø¨Ø¹Ø¨Ø¯ÙˆÙ† (150Ù…Â², 3 ØºØ±ÙØŒ 5 Ø³Ù†ÙˆØ§Øª)")
print(f"   ğŸ’° Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {predicted_price_1:,.0f} Ø¯ÙŠÙ†Ø§Ø±")

example_2 = pd.DataFrame({
    'Ø§Ù„Ù…Ø³Ø§Ø­Ø©_Ù…ØªØ±': [120],
    'Ø¹Ø¯Ø¯_Ø§Ù„ØºØ±Ù': [2],
    'Ø¹Ø¯Ø¯_Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª': [1],
    'Ø¹Ù…Ø±_Ø§Ù„Ø¨Ù†Ø§Ø¡_Ø³Ù†ÙˆØ§Øª': [15],
    'Ø·Ø§Ø¨Ù‚': [1],
    'ÙŠÙˆØ¬Ø¯_Ù…ØµØ¹Ø¯': [0],
    'ÙŠÙˆØ¬Ø¯_Ù…ÙˆÙ‚Ù': [0],
    'ÙŠÙˆØ¬Ø¯_Ø­Ø¯ÙŠÙ‚Ø©': [0],
    'ÙŠÙˆØ¬Ø¯_ØªØ¯ÙØ¦Ø©_Ù…Ø±ÙƒØ²ÙŠØ©': [0],
    'Ù‚Ø±Ø¨_Ø§Ù„Ø®Ø¯Ù…Ø§Øª': [5],
    'Ø§Ù„Ù…Ù†Ø·Ù‚Ø©_Ø±Ù‚Ù…': [le.transform(['Ù…Ø§Ø±ÙƒØ§'])[0]]
})

predicted_price_2 = best_model.predict(example_2)[0]
print(f"\nğŸ  Ù…Ø«Ø§Ù„ 2: Ø´Ù‚Ø© Ø¨Ù…Ø§Ø±ÙƒØ§ (120Ù…Â², 2 ØºØ±ÙØŒ 15 Ø³Ù†Ø©)")
print(f"   ğŸ’° Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {predicted_price_2:,.0f} Ø¯ÙŠÙ†Ø§Ø±")

print("\n" + "=" * 70)
print("âœ… Ø§Ù†ØªÙ‡Ù‰ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
print("=" * 70)